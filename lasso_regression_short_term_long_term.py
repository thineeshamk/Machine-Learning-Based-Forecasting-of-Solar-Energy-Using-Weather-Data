# -*- coding: utf-8 -*-
"""Lasso Regression Short Term Long Term.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j85hichAsLApXIttta4j8wAupFjXQDvq
"""

from google.colab import drive
drive.mount('/content/drive')

# Data manipulation
import pandas as pd
import numpy as np
import glob
import os

# Machine Learning
from sklearn.model_selection import train_test_split, GridSearchCV, TimeSeriesSplit
from sklearn.linear_model import Lasso
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Model saving
import joblib

# Suppress warnings for cleaner output
import warnings
warnings.filterwarnings('ignore')

# Define the path to the 'Lasso Regression' folder in Google Drive
data_path = '/content/drive/MyDrive/Lasso Regression/'  # Update if the folder is nested deeper

# Use glob to retrieve all CSV files in the directory
all_files = glob.glob(os.path.join(data_path, '*.csv'))

# Display the list of files found
print("CSV files found:")
for file in all_files:
    print(os.path.basename(file))

# Initialize an empty list to store individual DataFrames
df_list = []

# Iterate over each CSV file and append to the list
for file in all_files:
    try:
        # Read the CSV file
        df = pd.read_csv(file, parse_dates=['Time'], dayfirst=False)  # Adjust 'dayfirst' if needed

        # Append the DataFrame to the list
        df_list.append(df)

        print(f"Loaded {os.path.basename(file)} successfully.")
    except Exception as e:
        print(f"Error loading {os.path.basename(file)}: {e}")

# Concatenate all DataFrames into a single DataFrame
data = pd.concat(df_list, ignore_index=True)

# Display the shape of the combined DataFrame
print(f"\nCombined data shape: {data.shape}")

# Display the first few rows to verify
display(data.head())

# Check the data types
print("\nData types:")
print(data.dtypes)

# Ensure that the 'Time' column is in datetime format
if not np.issubdtype(data['Time'].dtype, np.datetime64):
    data['Time'] = pd.to_datetime(data['Time'], errors='coerce')
    print("\nConverted 'Time' column to datetime.")

# Display summary statistics
print("\nSummary statistics:")
display(data.describe())

# Check for missing values in the combined dataset
print("Missing values in each column:")
print(data.isnull().sum())

# Identify columns with missing values
missing_columns = data.columns[data.isnull().any()]
print("\nColumns with missing values:")
print(missing_columns)

# List of numerical columns
numerical_cols = ['TOAL ACTIVE POWER [MW]', 'Irradiation', 'Temp', 'Wind', 'Humidity', 'Barometer']

# Fill missing values in numerical columns with the median
for col in numerical_cols:
    if data[col].isnull().sum() > 0:
        median_value = data[col].median()
        data[col].fillna(median_value, inplace=True)
        print(f"Filled missing values in '{col}' with median value {median_value}.")

# List of categorical/weather columns
categorical_cols = [col for col in data.columns if 'Weather_' in col]

# Fill missing values in categorical/weather columns with 0
for col in categorical_cols:
    if data[col].isnull().sum() > 0:
        data[col].fillna(0, inplace=True)
        print(f"Filled missing values in '{col}' with 0.")

# Check for remaining missing values
print("\nMissing values after imputation:")
print(data.isnull().sum())

# Identify columns with missing values
missing_columns = data.columns[data.isnull().any()]
print("\nColumns with missing values:")
print(missing_columns)

# Import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import Lasso
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, make_scorer
from sklearn.preprocessing import StandardScaler
import glob
import os

# Display summary of the cleaned data
print("Cleaned Data Shape:", data.shape)
display(data.head())

# Create DataFrame copies for 15-minute and 60-minute ahead forecasts
df_15min = data.copy()
df_60min = data.copy()

# Shift the 'TOAL ACTIVE POWER [MW]' column to create target variables for forecasting
df_15min['Target_15min'] = df_15min['TOAL ACTIVE POWER [MW]'].shift(-15)
df_60min['Target_60min'] = df_60min['TOAL ACTIVE POWER [MW]'].shift(-60)

# Drop any rows with missing target values after shifting
df_15min = df_15min.dropna(subset=['Target_15min'])
df_60min = df_60min.dropna(subset=['Target_60min'])

# Display the shapes of the new DataFrames
print("15-minute ahead forecast data shape:", df_15min.shape)
print("60-minute ahead forecast data shape:", df_60min.shape)

# Define feature columns (exclude 'Time' and target columns)
feature_cols = ['Irradiation', 'Temp', 'Wind', 'Humidity', 'Barometer'] + [col for col in data.columns if 'Weather_' in col]

# Split data for 15-minute forecast
X_15min = df_15min[feature_cols]
y_15min = df_15min['Target_15min']

X_train_15min, X_test_15min, y_train_15min, y_test_15min = train_test_split(X_15min, y_15min, test_size=0.2, random_state=42)

# Split data for 60-minute forecast
X_60min = df_60min[feature_cols]
y_60min = df_60min['Target_60min']

X_train_60min, X_test_60min, y_train_60min, y_test_60min = train_test_split(X_60min, y_60min, test_size=0.2, random_state=42)

# Display shapes
print("Training data shapes:")
print("15-minute forecast:", X_train_15min.shape, y_train_15min.shape)
print("60-minute forecast:", X_train_60min.shape, y_train_60min.shape)

# Standardize the feature data for both forecasts
scaler = StandardScaler()

X_train_15min_scaled = scaler.fit_transform(X_train_15min)
X_test_15min_scaled = scaler.transform(X_test_15min)

X_train_60min_scaled = scaler.fit_transform(X_train_60min)
X_test_60min_scaled = scaler.transform(X_test_60min)

# Define Lasso model and hyperparameter grid
lasso_15min = Lasso(max_iter=10000)
param_grid = {'alpha': np.logspace(-4, 0, 50)}  # Range of alpha values

# Grid search with cross-validation
grid_search_15min = GridSearchCV(lasso_15min, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search_15min.fit(X_train_15min_scaled, y_train_15min)

# Best model
best_lasso_15min = grid_search_15min.best_estimator_

# Predictions
y_pred_15min = best_lasso_15min.predict(X_test_15min_scaled)

# Evaluation metrics
mse_15min = mean_squared_error(y_test_15min, y_pred_15min)
rmse_15min = np.sqrt(mse_15min)
mae_15min = mean_absolute_error(y_test_15min, y_pred_15min)
r2_15min = r2_score(y_test_15min, y_pred_15min)

# Display evaluation metrics
print("15-Minute Forecast Evaluation Metrics:")
print(f"MSE: {mse_15min:.4f}")
print(f"RMSE: {rmse_15min:.4f}")
print(f"MAE: {mae_15min:.4f}")
print(f"R^2: {r2_15min:.4f}")

print(f"Best alpha 15_min: {grid_search_15min.best_params_['alpha']}")

# Define Lasso model and hyperparameter grid
lasso_60min = Lasso(max_iter=10000)
grid_search_60min = GridSearchCV(lasso_60min, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search_60min.fit(X_train_60min_scaled, y_train_60min)

# Best model
best_lasso_60min = grid_search_60min.best_estimator_

# Predictions
y_pred_60min = best_lasso_60min.predict(X_test_60min_scaled)

# Evaluation metrics
mse_60min = mean_squared_error(y_test_60min, y_pred_60min)
rmse_60min = np.sqrt(mse_60min)
mae_60min = mean_absolute_error(y_test_60min, y_pred_60min)
r2_60min = r2_score(y_test_60min, y_pred_60min)

# Display evaluation metrics
print("60-Minute Forecast Evaluation Metrics:")
print(f"MSE: {mse_60min:.4f}")
print(f"RMSE: {rmse_60min:.4f}")
print(f"MAE: {mae_60min:.4f}")
print(f"R^2: {r2_60min:.4f}")

print(f"Best alpha 60_min: {grid_search_60min.best_params_['alpha']}")

from sklearn.model_selection import learning_curve

def plot_learning_curve(estimator, X, y, title):
    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)

    train_scores_mean = -np.mean(train_scores, axis=1)
    test_scores_mean = -np.mean(test_scores, axis=1)

    plt.figure(figsize=(10, 6))
    plt.plot(train_sizes, train_scores_mean, label='Training Error', marker='o', color='blue')
    plt.plot(train_sizes, test_scores_mean, label='Validation Error', marker='o', color='green')
    plt.title(f'Learning Curve: {title}')
    plt.xlabel('Training Size')
    plt.ylabel('Mean Squared Error')
    plt.legend(loc='upper right')
    plt.grid(True)
    plt.show()

# Plot learning curve for 15-minute forecast
plot_learning_curve(best_lasso_15min, X_train_15min_scaled, y_train_15min, '15-Minute Forecast')

# Plot learning curve for 60-minute forecast
plot_learning_curve(best_lasso_60min, X_train_60min_scaled, y_train_60min, '60-Minute Forecast')

from sklearn.model_selection import TimeSeriesSplit, GridSearchCV, cross_val_score
from sklearn.linear_model import Lasso
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.preprocessing import StandardScaler
import numpy as np

# --- Feature Engineering: Add lagged features for better predictions ---
def add_lagged_features(data, lag_steps):
    for lag in range(1, lag_steps + 1):
        data[f'Lag_{lag}'] = data['TOAL ACTIVE POWER [MW]'].shift(lag)
    return data

# Add lagged features for both datasets
df_15min = add_lagged_features(df_15min, lag_steps=5)  # 5 lagged steps
df_60min = add_lagged_features(df_60min, lag_steps=5)  # 5 lagged steps

# Drop rows with NaN values introduced by lagged features
df_15min.dropna(inplace=True)
df_60min.dropna(inplace=True)

# Updated feature columns to include lagged features
feature_cols = [col for col in df_15min.columns if col.startswith('Lag_')] + [
    'Irradiation', 'Temp', 'Wind', 'Humidity', 'Barometer'
] + [col for col in df_15min.columns if 'Weather_' in col]

# --- Split Data for 15-minute and 60-minute Forecasts ---
X_15min = df_15min[feature_cols]
y_15min = df_15min['Target_15min']

X_60min = df_60min[feature_cols]
y_60min = df_60min['Target_60min']

# Train-Test Split
X_train_15min, X_test_15min, y_train_15min, y_test_15min = train_test_split(
    X_15min, y_15min, test_size=0.2, shuffle=False
)
X_train_60min, X_test_60min, y_train_60min, y_test_60min = train_test_split(
    X_60min, y_60min, test_size=0.2, shuffle=False
)

# Standardize Data
scaler_15min = StandardScaler()
X_train_15min_scaled = scaler_15min.fit_transform(X_train_15min)
X_test_15min_scaled = scaler_15min.transform(X_test_15min)

scaler_60min = StandardScaler()
X_train_60min_scaled = scaler_60min.fit_transform(X_train_60min)
X_test_60min_scaled = scaler_60min.transform(X_test_60min)

# --- Define Models and Improved Hyperparameter Tuning ---
lasso_15min_2 = Lasso(max_iter=10000)
lasso_60min_2 = Lasso(max_iter=10000)

# Use TimeSeriesSplit for better handling of time series data
tscv = TimeSeriesSplit(n_splits=5)

# Grid Search for hyperparameter tuning
param_grid = {'alpha': np.logspace(-5, 1, 100)}  # Expand the search space for alpha

# Grid search with cross-validation for the 15-minute model
grid_search_15min_2 = GridSearchCV(
    lasso_15min_2, param_grid, cv=tscv, scoring='neg_mean_squared_error', n_jobs=-1
)
grid_search_15min_2.fit(X_train_15min_scaled, y_train_15min)

# Best model for 15-minute forecast
lasso_15min_2_best = grid_search_15min_2.best_estimator_

# Grid search with cross-validation for the 60-minute model
grid_search_60min_2 = GridSearchCV(
    lasso_60min_2, param_grid, cv=tscv, scoring='neg_mean_squared_error', n_jobs=-1
)
grid_search_60min_2.fit(X_train_60min_scaled, y_train_60min)

# Best model for 60-minute forecast
lasso_60min_2_best = grid_search_60min_2.best_estimator_

# --- Model Evaluation ---
def evaluate_model(model, X_test, y_test, model_name):
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    print(f"{model_name} Evaluation Metrics:")
    print(f"MSE: {mse:.4f}")
    print(f"RMSE: {rmse:.4f}")
    print(f"MAE: {mae:.4f}")
    print(f"R^2: {r2:.4f}")

    return y_pred

# Evaluate 15-minute forecast model
y_pred_15min_2 = evaluate_model(lasso_15min_2_best, X_test_15min_scaled, y_test_15min, "Improved 15-Minute Forecast")

# Evaluate 60-minute forecast model
y_pred_60min_2 = evaluate_model(lasso_60min_2_best, X_test_60min_scaled, y_test_60min, "Improved 60-Minute Forecast")

# --- Visualizations ---
# Actual vs Predicted for Test Set
def plot_actual_vs_predicted(y_test, y_pred, title):
    plt.figure(figsize=(10, 6))
    plt.plot(y_test.values[:3000], label="Actual", color="blue", alpha=0.6)
    plt.plot(y_pred[:3000], label="Predicted", color="red", alpha=0.6)
    plt.title(title)
    plt.xlabel("Time")
    plt.ylabel("Active Power (MW)")
    plt.legend()
    plt.grid(True)
    plt.show()

# Plot results for 15-minute forecast
plot_actual_vs_predicted(y_test_15min, y_pred_15min_2, "15-Minute Forecast: Actual vs Predicted")

# Plot results for 60-minute forecast
plot_actual_vs_predicted(y_test_60min, y_pred_60min_2, "60-Minute Forecast: Actual vs Predicted")

print(f"Best alpha_2 15_min: {grid_search_15min_2.best_params_['alpha']}")
print(f"Best alpha_2 60_min: {grid_search_60min_2.best_params_['alpha']}")

# Function to plot learning curve
def plot_learning_curve(model, X_train, y_train, title):
    train_sizes, train_scores, test_scores = learning_curve(
        model, X_train, y_train, cv=5, scoring='neg_mean_squared_error', n_jobs=-1
    )
    train_scores_mean = -np.mean(train_scores, axis=1)
    test_scores_mean = -np.mean(test_scores, axis=1)

    plt.figure(figsize=(10, 6))
    plt.plot(train_sizes, train_scores_mean, label='Training Error', color='blue', marker='o')
    plt.plot(train_sizes, test_scores_mean, label='Validation Error', color='green', marker='o')
    plt.title(f"Learning Curve: {title}")
    plt.xlabel("Training Size")
    plt.ylabel("Mean Squared Error")
    plt.legend(loc="upper right")
    plt.grid(True)
    plt.show()

# Function to plot actual vs. predicted for the test set
def plot_actual_vs_predicted(y_test, y_pred, title, subset_size=None):
    if subset_size:
        y_test = y_test[:subset_size]
        y_pred = y_pred[:subset_size]
    plt.figure(figsize=(12, 6))
    plt.plot(range(len(y_test)), y_test, label="Actual Power", color="blue", alpha=0.7)
    plt.plot(range(len(y_pred)), y_pred, label="Predicted Power", color="orange", linestyle="dashed")
    plt.xlabel("Sample Index")
    plt.ylabel("Power Output")
    plt.title(title)
    plt.legend()
    plt.grid(True)
    plt.show()

# Function to plot feature importance
def plot_feature_importance(model, feature_names, title):
    coefficients = model.coef_
    important_features_indices = np.where(coefficients != 0)[0]
    important_features = [feature_names[i] for i in important_features_indices]
    important_coefficients = coefficients[important_features_indices]

    # Sort by absolute importance for better visualization
    sorted_indices = np.argsort(np.abs(important_coefficients))[::-1]
    sorted_features = [important_features[i] for i in sorted_indices]
    sorted_coefficients = important_coefficients[sorted_indices]

    plt.figure(figsize=(10, 6))
    plt.barh(sorted_features, sorted_coefficients, color="skyblue")
    plt.xlabel("Coefficient Value")
    plt.ylabel("Feature")
    plt.title(title)
    plt.grid(True)
    plt.show()

# Evaluate and plot for each model
feature_names = feature_cols  # Feature names

# **Model 1: 15-Minute Forecast**
plot_learning_curve(lasso_15min_2_best, X_train_15min_scaled, y_train_15min, "15-Minute Forecast") # Use the fitted model: lasso_15min_2_best
plot_actual_vs_predicted(y_test_15min, y_pred_15min_2, "Actual vs Predicted: 15-Minute Forecast", subset_size=3000)
plot_feature_importance(lasso_15min_2_best, feature_names, "Feature Importance: 15-Minute Forecast") # Use the fitted model: lasso_15min_2_best

# **Model 2: 60-Minute Forecast**
plot_learning_curve(lasso_60min_2_best, X_train_60min_scaled, y_train_60min, "60-Minute Forecast") # Use the fitted model: lasso_60min_2_best
plot_actual_vs_predicted(y_test_60min, y_pred_60min_2, "Actual vs Predicted: 60-Minute Forecast", subset_size=3000)
plot_feature_importance(lasso_60min_2_best, feature_names, "Feature Importance: 60-Minute Forecast") # Use the fitted model: lasso_60min_2_best

from sklearn.model_selection import learning_curve
import numpy as np

# Plot the Learning Curve for the 15-Minute Model
train_sizes, train_scores, test_scores = learning_curve(
    lasso_15min_2_best, X_train_15min_scaled, y_train_15min,
    cv=5, scoring='neg_mean_squared_error', train_sizes=np.linspace(0.1, 1.0, 10)
)

# Convert scores to RMSE
train_rmse = np.sqrt(-train_scores.mean(axis=1))
test_rmse = np.sqrt(-test_scores.mean(axis=1))

plt.figure(figsize=(10, 6))
plt.plot(train_sizes, train_rmse, marker='o', label='Training RMSE')
plt.plot(train_sizes, test_rmse, marker='o', label='Testing RMSE')
plt.xlabel('Training Set Size')
plt.ylabel('RMSE')
plt.title('Learning Curve (15-Minute Forecast)')
plt.legend()
plt.grid(True)
plt.show()

# Plot the Learning Curve for the 60-Minute Model
train_sizes, train_scores, test_scores = learning_curve(
    lasso_60min_2_best, X_train_60min_scaled, y_train_60min,
    cv=5, scoring='neg_mean_squared_error', train_sizes=np.linspace(0.1, 1.0, 10)
)

# Convert scores to RMSE
train_rmse = np.sqrt(-train_scores.mean(axis=1))
test_rmse = np.sqrt(-test_scores.mean(axis=1))

plt.figure(figsize=(10, 6))
plt.plot(train_sizes, train_rmse, marker='o', label='Training RMSE')
plt.plot(train_sizes, test_rmse, marker='o', label='Testing RMSE')
plt.xlabel('Training Set Size')
plt.ylabel('RMSE')
plt.title('Learning Curve (60-Minute Forecast)')
plt.legend()
plt.grid(True)
plt.show()

# Plot Feature Importance for 15-Minute Model
coefficients = lasso_15min_2_best.coef_
important_features_indices = np.where(coefficients != 0)[0]
important_features = [feature_names[i] for i in important_features_indices]

plt.figure(figsize=(10, 6))
plt.barh(important_features, coefficients[important_features_indices])
plt.xlabel('Coefficient Value')
plt.ylabel('Feature')
plt.title('Feature Importance (15-Minute Forecast)')
plt.grid(True)
plt.show()

# Plot Feature Importance for 60-Minute Model
coefficients = lasso_60min_2_best.coef_
important_features_indices = np.where(coefficients != 0)[0]
important_features = [feature_names[i] for i in important_features_indices]

plt.figure(figsize=(10, 6))
plt.barh(important_features, coefficients[important_features_indices])
plt.xlabel('Coefficient Value')
plt.ylabel('Feature')
plt.title('Feature Importance (60-Minute Forecast)')
plt.grid(True)
plt.show()

# --- Visualizations ---
# Actual vs Predicted for Test Set
def plot_actual_vs_predicted_full(y_test, y_pred, title):
    plt.figure(figsize=(10, 6))
    plt.plot(y_test.values, label="Actual", color="blue", alpha=0.6)
    plt.plot(y_pred, label="Predicted", color="red", alpha=0.6)
    plt.title(title)
    plt.xlabel("Time")
    plt.ylabel("Active Power (MW)")
    plt.legend()
    plt.grid(True)
    plt.show()

# Plot results for 15-minute forecast (Entire Test Set)
plot_actual_vs_predicted_full(y_test_15min, y_pred_15min_2, "15-Minute Forecast: Actual vs Predicted")

# Plot results for 60-minute forecast (Entire Test Set)
plot_actual_vs_predicted_full(y_test_60min, y_pred_60min_2, "60-Minute Forecast: Actual vs Predicted")